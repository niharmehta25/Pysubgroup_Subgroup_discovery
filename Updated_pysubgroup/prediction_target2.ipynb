{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60154ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numbers\n",
    "from functools import total_ordering\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "\n",
    "import pysubgroup as ps\n",
    "\n",
    "@total_ordering\n",
    "class PredictionTarget:\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_variable,\n",
    "            target_estimate):\n",
    "\n",
    "        self.target_variable = target_variable\n",
    "        self.target_estimate = target_estimate\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"PredictionTarget[\" +\\\n",
    "            f\"values: {PredictionTarget.get_target_descriptor(self.target_variable)}; \" +\\\n",
    "            f\"estimates: {PredictionTarget.get_target_descriptor(self.target_estimate)}]\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_target_descriptor(target):\n",
    "        if isinstance(target, np.ndarray):\n",
    "            return f\"ndarray (len={target.shape[0]})\"\n",
    "        elif isinstance(target, (pd.Series, pd.DataFrame)):\n",
    "            return f\"{type(target)} (len={len(target)})\"\n",
    "        # TODO: be more specific about target selection?\n",
    "        else:\n",
    "            return f\"'{str(target)}'\"\n",
    "\n",
    "\n",
    "class PredictionTargetStatisticsCalculator(ps.CombinedStatisticsCalculator):\n",
    "\n",
    "    TARGET_STATISTICS = {\n",
    "        \"classification\": collections.OrderedDict([\n",
    "            (\"pos\", lambda y, _: np.sum(y)),\n",
    "            (\"neg\", lambda y, _: len(y) - np.sum(y)),\n",
    "            (\"roc_auc\",   sklearn.metrics.roc_auc_score),\n",
    "            (\"avg_prec\",  sklearn.metrics.average_precision_score),\n",
    "            (\"precision\", lambda y, y_pred: sklearn.metrics.precision_score(y, y_pred > 0.5)),\n",
    "            (\"recall\",    lambda y, y_pred: sklearn.metrics.recall_score(y, y_pred > 0.5)),\n",
    "        ]),\n",
    "        \"regression\": collections.OrderedDict([\n",
    "            (\"y_mean\",     lambda y, _: np.mean(y)),\n",
    "            (\"y_std\",      lambda y, _: np.std(y)),\n",
    "            (\"y_median\",   lambda y, _: np.median(y)),\n",
    "            (\"y_mean\",     lambda _, y_pred: np.mean(y_pred)),\n",
    "            (\"y_std\",      lambda _, y_pred: np.std(y_pred)),\n",
    "            (\"y_median\",   lambda _, y_pred: np.median(y_pred)),\n",
    "            (\"pearson_r\",  lambda y, y_pred: scipy.stats.pearsonr(y, y_pred)[0]),\n",
    "            (\"pearson_p\",  lambda y, y_pred: scipy.stats.pearsonr(y, y_pred)[1]),\n",
    "            (\"spearman_r\", lambda y, y_pred: scipy.stats.spearmanr(y, y_pred)[0]),\n",
    "            (\"spearman_p\", lambda y, y_pred: scipy.stats.spearmanr(y, y_pred)[1]),\n",
    "            (\"mae\",        sklearn.metrics.mean_absolute_error),\n",
    "            (\"mse\",        sklearn.metrics.mean_squared_error),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            target_variable,\n",
    "            target_estimate,\n",
    "            target_statistics,\n",
    "            handle_statistics_errors=np.nan,\n",
    "            dual_statistics=True) -> None:\n",
    "\n",
    "        base_statistics_calculator = ps.BaseStatisticsCalculator()\n",
    "\n",
    "        if isinstance(target_statistics, str):\n",
    "             target_statistics = PredictionTargetStatisticsCalculator.TARGET_STATISTICS[target_statistics]\n",
    "\n",
    "        def wrap_statistics_func(func):\n",
    "            def wrapped(subgroup, df_sg, df):\n",
    "                return func(\n",
    "                    PredictionTargetStatisticsCalculator.get_values(target_variable, df_sg), \n",
    "                    PredictionTargetStatisticsCalculator.get_values(target_estimate, df_sg))\n",
    "            return wrapped\n",
    "\n",
    "        wrapped_statistics = collections.OrderedDict([\n",
    "            (stat_name, wrap_statistics_func(stat_func))\n",
    "            for stat_name, stat_func in target_statistics.items()])\n",
    "\n",
    "        target_statistics_calculator = ps.GenericStatisticsCalculator(\n",
    "            wrapped_statistics, \n",
    "            handle_statistics_errors=handle_statistics_errors, \n",
    "            dual_statistics=dual_statistics)\n",
    "\n",
    "        self.statistics = target_statistics\n",
    "        self.wrapped_statistics = wrapped_statistics\n",
    "        super().__init__(base_statistics_calculator, target_statistics_calculator)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_values(target, data):\n",
    "        \"\"\"TODO: should we drop this and force data frames?\"\"\"\n",
    "        if isinstance(target, np.ndarray):\n",
    "            return target\n",
    "        elif isinstance(target, (pd.Series, pd.DataFrame)):\n",
    "            return target.values()\n",
    "        # TODO: be more specific about target selection?\n",
    "        else:\n",
    "            return data[target].values\n",
    "\n",
    "    @staticmethod\n",
    "    def from_target(\n",
    "            target: PredictionTarget,\n",
    "            target_statistics,\n",
    "            handle_statistics_errors=np.nan,\n",
    "            dual_statistics=True):\n",
    "        return PredictionTargetStatisticsCalculator(\n",
    "            target_variable=target.target_variable,\n",
    "            target_estimate=target.target_estimate,\n",
    "            target_statistics=target_statistics,\n",
    "            handle_statistics_errors=handle_statistics_errors,\n",
    "            dual_statistics=dual_statistics)\n",
    "\n",
    "\n",
    "class PredictionQFNumeric(ps.BoundedInterestingnessMeasure):\n",
    "\n",
    "    tpl = collections.namedtuple('PredictionQFNumeric_parameters', ('size', 'metric'))\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            metric,\n",
    "            metric_transform=None,\n",
    "            size_exponent=1,\n",
    "            relative_subgroup_size=True,\n",
    "            relative_subgroup_metric='none',\n",
    "            optimistic_estimate=None,\n",
    "            handle_metric_error='raise',\n",
    "            warn_on_metric_error=True,\n",
    "            prune_lower_than_dataset_metric=False):\n",
    "\n",
    "        if not isinstance(size_exponent, numbers.Number):\n",
    "            raise ValueError(f'a is not a number. Received a={size_exponent}')\n",
    "\n",
    "        self.metric = metric\n",
    "        self.metric_transform = metric_transform\n",
    "        self.size_exponent = size_exponent\n",
    "\n",
    "        self.relative_subgroup_size = relative_subgroup_size\n",
    "        self.relative_subgroup_metric = relative_subgroup_metric\n",
    "\n",
    "        if optimistic_estimate is None:\n",
    "            self.estimator = PredictionQFNumeric.MaxValueOptimisticEstimator(float(\"inf\"))\n",
    "        elif isinstance(size_exponent, numbers.Number):\n",
    "            self.estimator = PredictionQFNumeric.MaxValueOptimisticEstimator(optimistic_estimate)\n",
    "        else:\n",
    "            self.estimator =  optimistic_estimate\n",
    "\n",
    "        self.handle_metric_error = handle_metric_error\n",
    "        self.warn_on_metric_error = warn_on_metric_error\n",
    "\n",
    "        self.prune_lower_than_dataset_metric = prune_lower_than_dataset_metric\n",
    "\n",
    "        self.required_stat_attrs = ('size_sg', 'metric_sg')\n",
    "\n",
    "        # place holders for statistics\n",
    "        self.dataset_statistics = None\n",
    "        self.has_constant_statistics = False\n",
    "\n",
    "    def calculate_constant_statistics(self, data, target: PredictionTarget):\n",
    "        \"\"\"\n",
    "        Calculate statistics that do not change during the search and can thus be calculated before the actual search starts.\n",
    "        \"\"\"\n",
    "\n",
    "        size_dataset = len(data)\n",
    "\n",
    "        target_variable_dataset = PredictionTargetStatisticsCalculator.get_values(target.target_variable, data)\n",
    "        target_estimate_dataset = PredictionTargetStatisticsCalculator.get_values(target.target_estimate, data)\n",
    "\n",
    "        target_metric_dataset = self.calculate_metric(\n",
    "            self.metric, target_variable_dataset, target_estimate_dataset)\n",
    "        self.dataset_statistics = PredictionQFNumeric.tpl(size_dataset, target_metric_dataset)\n",
    "        \n",
    "        self.has_constant_statistics = True\n",
    "\n",
    "    def calculate_statistics(self, subgroup, target, data, statistics=None):\n",
    "        \n",
    "        # TODO: ensure constant statistics\n",
    "        size_dataset, _ = self.dataset_statistics\n",
    "\n",
    "        cover_arr, size_sg = ps.get_cover_array_and_size(\n",
    "            subgroup, data_len=size_dataset, data=data)\n",
    "\n",
    "        # target values\n",
    "        target_values_dataset = PredictionTargetStatisticsCalculator.get_values(target.target_variable, data)\n",
    "        target_values_sg = target_values_dataset[cover_arr]\n",
    "\n",
    "        # target estimates\n",
    "        target_estimates_dataset = PredictionTargetStatisticsCalculator.get_values(target.target_estimate, data)\n",
    "        target_estimates_sg = target_estimates_dataset[cover_arr]\n",
    "\n",
    "        # calculate subgroup metric\n",
    "        metric_sg = self.calculate_metric(\n",
    "            self.metric, target_values_sg, target_estimates_sg)\n",
    "\n",
    "        return PredictionQFNumeric.tpl(size_sg, metric_sg)\n",
    "\n",
    "    def evaluate(self, subgroup, target, data, statistics=None):\n",
    "\n",
    "        statistics_sg = self.ensure_statistics(subgroup, target, data, statistics)\n",
    "        statistics_dataset = self.dataset_statistics\n",
    "\n",
    "        q = PredictionQFNumeric.prediction_qf_numeric(\n",
    "            size_sg=statistics_sg.size,\n",
    "            metric_sg=statistics_sg.metric,\n",
    "            size_dataset=statistics_dataset.size,\n",
    "            metric_dataset=statistics_dataset.metric,\n",
    "            size_exponent=self.size_exponent,\n",
    "            metric_transform=self.metric_transform,\n",
    "            relative_metric=self.relative_subgroup_metric,\n",
    "            relative_size=self.relative_subgroup_size,\n",
    "            prune_lower_than_dataset_metric=self.prune_lower_than_dataset_metric)\n",
    "            \n",
    "        return q\n",
    "\n",
    "    def optimistic_estimate(self, subgroup, target, data, statistics=None):\n",
    "        \n",
    "        statistics_sg = self.ensure_statistics(subgroup, target, data, statistics)\n",
    "        statistics_dataset = self.dataset_statistics\n",
    "\n",
    "        # TODO: for some reason in other implementations calling the estimator is moved to `calculate_statistics`; efficiency? would the same apply for `evaluate`?\n",
    "        return self.estimator.get_estimate(\n",
    "            size_sg=statistics_sg.size,\n",
    "            metric_sg=statistics_sg.metric,\n",
    "            size_dataset=statistics_dataset.size,\n",
    "            metric_dataset=statistics_dataset.metric,\n",
    "            size_exponent=self.size_exponent,\n",
    "            metric_transform=self.metric_transform,\n",
    "            relative_metric=self.relative_subgroup_metric,\n",
    "            relative_size=self.relative_subgroup_size)\n",
    "\n",
    "    def calculate_metric(self, metric, target_values, target_estimate):\n",
    "        try:\n",
    "            return metric(target_values, target_estimate)\n",
    "        except Exception as e:\n",
    "            if self.handle_metric_error == \"raise\":\n",
    "                raise e\n",
    "            else:\n",
    "                if self.warn_on_metric_error:\n",
    "                    warnings.warn(f\"Error while evaluating metric: {e}\")\n",
    "                return self.handle_metric_error\n",
    "\n",
    "    @staticmethod\n",
    "    def prediction_qf_numeric(\n",
    "            size_sg,\n",
    "            metric_sg,\n",
    "            size_dataset=None,\n",
    "            metric_dataset=None,\n",
    "            size_exponent=1,\n",
    "            metric_transform=None,  # 'reverse', 'invert', or callable\n",
    "            relative_size=True,\n",
    "            # TODO: merge with `metric_transform`?\n",
    "            relative_metric='none',  # 'ratio' or 'difference'\n",
    "            prune_lower_than_dataset_metric=False\n",
    "        ):\n",
    "\n",
    "        if relative_size:\n",
    "            size = size_sg / size_dataset\n",
    "\n",
    "        if relative_metric == \"none\":\n",
    "            metric = metric_sg\n",
    "        elif relative_metric == \"ratio\":\n",
    "            metric = metric_sg / metric_dataset\n",
    "        elif relative_metric == \"difference\":\n",
    "            metric = metric_sg - metric_dataset\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown relative metric mode: {relative_metric}\")\n",
    "\n",
    "        if prune_lower_than_dataset_metric:\n",
    "            if metric_sg < metric_dataset:\n",
    "                return float(\"-inf\")\n",
    "\n",
    "        # transfrom metric\n",
    "        # TODO: reevaluate transform situation!\n",
    "        if metric_transform is None:\n",
    "            metric = metric_sg\n",
    "\n",
    "        # elif isinstance(metric_transform, str) and metric_transform.startswith(\"invert\"):\n",
    "        #     if metric != 0:\n",
    "        #         metric = 1.0 / metric_sg\n",
    "        #     else:\n",
    "        #         if metric_transform.endswith(\"invert\") or metric_transform.endwith(\"+\"):\n",
    "        #             metric = float(\"inf\") # TODO: when metric_sg = 0 and inverted just return inf, this assumes low metric is bad\n",
    "        #         elif metric_transform.endswith(\"-\"):\n",
    "        #             metric = float(\"-inf\")\n",
    "        #         else:\n",
    "        #             raise ValueError(f\"Unknown inversion command: {metric_transform}\")\n",
    "\n",
    "        elif callable(metric_transform):\n",
    "            metric = metric_transform(metric_sg, metric_dataset)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric transform: {metric_transform}\")\n",
    "\n",
    "        # calculate quality\n",
    "        return size ** size_exponent * metric\n",
    "        \n",
    "\n",
    "    class MaxValueOptimisticEstimator:\n",
    "        def __init__(self, value):\n",
    "            self.max_value = value\n",
    "\n",
    "        def get_data(self, data):\n",
    "            return data\n",
    "\n",
    "        def get_estimate(\n",
    "                self,\n",
    "                size_sg,\n",
    "                metric_sg,\n",
    "                size_dataset=1,\n",
    "                metric_dataset=1,\n",
    "                size_exponent=1,\n",
    "                metric_transform=None,\n",
    "                relative_size=True, \n",
    "                relative_metric=False):\n",
    "            \n",
    "            return PredictionQFNumeric.prediction_qf_numeric(\n",
    "                size_sg=size_sg,\n",
    "                metric_sg=self.max_value,\n",
    "                size_dataset=size_dataset,\n",
    "                metric_dataset=metric_dataset,\n",
    "                size_exponent=size_exponent,\n",
    "                metric_transform=metric_transform,\n",
    "                relative_metric=relative_metric,\n",
    "                relative_size=relative_size,\n",
    "                prune_lower_than_dataset_metric=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
